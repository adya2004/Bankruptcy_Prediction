{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <strong>Knowledge Graph Embeddings</strong>\n",
    "\n",
    "We generate embeddings for the knowledge graph created earlier.\n",
    "\n",
    "To keep things simple, we use json files creathd to access the triples, rather then quering from neo4j api."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Information** :\n",
    "\n",
    "We are using different models to from *pykeen* library to generate embeddings from the knowledge graph triples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from pykeen.pipeline import pipeline\n",
    "from pykeen.triples import TriplesFactory\n",
    "from collections import defaultdict\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import json\n",
    "import pykeen\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import logging\n",
    "import pandas as pd\n",
    "\n",
    "torch.manual_seed(43)\n",
    "\n",
    "# Set the logging level to ERROR\n",
    "logging.getLogger(\"pykeen\").setLevel(logging.ERROR)\n",
    "logging.getLogger(\"torch\").setLevel(logging.ERROR)\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check cuda availability\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set files path \n",
    "bankrupt_json_path = r'output\\bankrupt'\n",
    "healthy_json_path = r'output\\healthy'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "Create **Entity2id** and **Relation2id** dict to effectivly number and convert any entitiy into numerical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 50 bankrupt company json files\n",
      "Found 50 healthy company json files\n"
     ]
    }
   ],
   "source": [
    "bankrupt_json_file = os.listdir(bankrupt_json_path)\n",
    "healthy_json_file = os.listdir(healthy_json_path)\n",
    "\n",
    "print(f'Found {bankrupt_json_file.__len__()} bankrupt company json files')\n",
    "print(f'Found {healthy_json_file.__len__()} healthy company json files')\n",
    "\n",
    "# initialize dictionaries for entities and relations to ids\n",
    "entity2id = defaultdict(lambda: len(entity2id))\n",
    "relation2id = defaultdict(lambda: len(relation2id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "Function to train model for generating embeddings using different models like\n",
    "<ul>\n",
    "<li>TransE</li>\n",
    "<li>RotatE (cann't use because of complex numbers)</li>\n",
    "<li>TransH</li>\n",
    "<li>ConvE</li>\n",
    "<li>RGCN</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################\n",
    "# 16/12/24 | 11:15 AM | \n",
    "# Aggregation of embeddings is done by taking mean of all the embeddings\n",
    "# each triples creates one embeddings, these embeddings are then averaged\n",
    "#****IMPROVMENTS***###########################################\n",
    "# 1. Add more models.\n",
    "# 2. Proper test-train split\n",
    "##############################################################\n",
    "all_json_files = []\n",
    "\n",
    "for file in bankrupt_json_file:\n",
    "    all_json_files.append(os.path.join(bankrupt_json_path, file))\n",
    "\n",
    "for file in healthy_json_file:\n",
    "    all_json_files.append(os.path.join(healthy_json_path, file))\n",
    "\n",
    "def train_model_once(files, model=\"TransE\", embedding_dim=10, split_ratio=[0.8, 0.2]):\n",
    "    \"\"\"\n",
    "    Train the model on a unified dataset from multiple files.\n",
    "    Returns the trained model and entity/relation mappings.\n",
    "    This function makes sure the model is trained only once.\n",
    "    \"\"\"\n",
    "    all_triples = []\n",
    "\n",
    "    # Combine triples from all files\n",
    "    for json_file in files:\n",
    "        with open(json_file, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "            for relation in data[\"relations\"]:\n",
    "                entity2id[relation[\"source\"]]\n",
    "                relation2id[relation[\"relation\"]]\n",
    "                entity2id[relation[\"target\"]]\n",
    "                all_triples.append(\n",
    "                    (relation[\"source\"], relation[\"relation\"], relation[\"target\"])\n",
    "                )\n",
    "\n",
    "    # Create a unified triples factory\n",
    "    triples_factory = TriplesFactory.from_labeled_triples(np.array(all_triples))\n",
    "\n",
    "    # Split into training and testing\n",
    "    try:\n",
    "        training_factory, testing_factory = triples_factory.split(split_ratio)\n",
    "    except Exception as e:\n",
    "        training_factory = triples_factory\n",
    "        testing_factory = triples_factory\n",
    "\n",
    "    # Train the model\n",
    "    result = pipeline(\n",
    "        model=model,\n",
    "        dataset=None,\n",
    "        training=training_factory,\n",
    "        testing=testing_factory,\n",
    "        model_kwargs=dict(embedding_dim=embedding_dim),\n",
    "    )\n",
    "\n",
    "    return result.model, triples_factory\n",
    "\n",
    "def generate_embeddings_for_file(json_file, model, triples_factory):\n",
    "    \"\"\"\n",
    "    Generates embeddings for a single file using a pre-trained model.\n",
    "    Input:\n",
    "    - json_file: path to the JSON file\n",
    "    - model: trained model\n",
    "    - triples_factory: triples factory object\n",
    "    Output:\n",
    "    - company_embedding: aggregated embedding for the company\n",
    "    \"\"\"\n",
    "    ent_emb = {}\n",
    "    rel_emb = {}\n",
    "\n",
    "    with open(json_file, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "        triples = []\n",
    "        for relation in data[\"relations\"]:\n",
    "            triples.append((relation[\"source\"], relation[\"relation\"], relation[\"target\"]))\n",
    "\n",
    "    # Extract entity and relation embeddings\n",
    "    for i, content in enumerate(model.entity_representations[0]().detach().numpy()):\n",
    "        ent_emb[triples_factory.entity_labeling.id_to_label[i]] = content\n",
    "\n",
    "    for i, content in enumerate(model.relation_representations[0]().detach().numpy()):\n",
    "        rel_emb[triples_factory.relation_labeling.id_to_label[i]] = content\n",
    "\n",
    "    # Aggregate embeddings\n",
    "    def aggregate():\n",
    "        store = [\n",
    "            np.concatenate([\n",
    "                ent_emb[s],\n",
    "                rel_emb[r],\n",
    "                ent_emb[t]\n",
    "            ]) for s, r, t in triples\n",
    "        ]\n",
    "        return np.mean(store, axis=0)\n",
    "\n",
    "    company_embedding = aggregate()\n",
    "    return company_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################\n",
    "# 5 Models used:\n",
    "# TransE\n",
    "# RotatE\n",
    "# TransH\n",
    "# ConvE\n",
    "# RGCN\n",
    "##############################################################\n",
    "model_te, triple_factory_te = train_model_once(all_json_files)\n",
    "model_re, triple_factory_re = train_model_once(all_json_files, model=\"RotatE\")\n",
    "model_th, triple_factory_th = train_model_once(all_json_files, model=\"TransH\")\n",
    "model_ce, triple_factory_ce = train_model_once(all_json_files, model=\"ConvE\")\n",
    "model_rgcn, triple_factory_rgcn = train_model_once(all_json_files, model=\"RGCN\")\n",
    "##############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################\n",
    "# 16/12/24 | 10:15 PM | \n",
    "# generating embeddings for all companies\n",
    "# files are in order so the labels are 50 bankrupt [1] and 50 healthy \n",
    "##############################################################\n",
    "all_company_data_te = []\n",
    "\n",
    "for files in all_json_files:\n",
    "    all_company_data_te.append(generate_embeddings_for_file(files, model_te, triple_factory_te))\n",
    "\n",
    "label_te = [50*[1] + 50*[0]]\n",
    "\n",
    "all_company_data_re = []\n",
    "\n",
    "for files in all_json_files:\n",
    "    all_company_data_re.append(generate_embeddings_for_file(files, model_re, triple_factory_re))\n",
    "\n",
    "label_re = [50*[1] + 50*[0]]\n",
    "\n",
    "all_company_data_th = []\n",
    "\n",
    "for files in all_json_files:\n",
    "    all_company_data_th.append(generate_embeddings_for_file(files, model_th, triple_factory_th))\n",
    "\n",
    "label_th = [50*[1] + 50*[0]]\n",
    "\n",
    "all_company_data_ce = []\n",
    "\n",
    "for files in all_json_files:\n",
    "    all_company_data_ce.append(generate_embeddings_for_file(files, model_ce, triple_factory_ce))\n",
    "\n",
    "label_ce = [50*[1] + 50*[0]]\n",
    "\n",
    "all_company_data_rgcn = []\n",
    "\n",
    "for files in all_json_files:\n",
    "    all_company_data_rgcn.append(generate_embeddings_for_file(files, model_rgcn, triple_factory_rgcn))\n",
    "\n",
    "label_rgcn = [50*[1] + 50*[0]]\n",
    "\n",
    "assert len(all_company_data_te) == bankrupt_json_file.__len__() + healthy_json_file.__len__()\n",
    "assert len(all_company_data_re) == bankrupt_json_file.__len__() + healthy_json_file.__len__()\n",
    "assert len(all_company_data_th) == bankrupt_json_file.__len__() + healthy_json_file.__len__()\n",
    "assert len(all_company_data_ce) == bankrupt_json_file.__len__() + healthy_json_file.__len__()\n",
    "assert len(all_company_data_rgcn) == bankrupt_json_file.__len__() + healthy_json_file.__len__()\n",
    "##############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################\n",
    "# 16/12/24 | 10:15 PM | \n",
    "# Normalizing the embeddings\n",
    "# create a databaset for each model\n",
    "##############################################################\n",
    "normalized_embedding_te = scaler.fit_transform(all_company_data_te)\n",
    "normalized_embedding_th = scaler.fit_transform(all_company_data_th)\n",
    "normalized_embedding_ce = scaler.fit_transform(all_company_data_ce)\n",
    "normalized_embedding_rgcn = scaler.fit_transform(all_company_data_rgcn)\n",
    "\n",
    "X_te = np.array(normalized_embedding_te)\n",
    "y_te = np.array(label_te).flatten()\n",
    "\n",
    "X_re = np.array(all_company_data_re)\n",
    "y_re = np.array(label_re).flatten()\n",
    "\n",
    "X_th = np.array(normalized_embedding_th)\n",
    "y_th = np.array(label_th).flatten()\n",
    "\n",
    "X_ce = np.array(normalized_embedding_ce)\n",
    "y_ce = np.array(label_ce).flatten()\n",
    "\n",
    "X_rgcn = np.array(normalized_embedding_rgcn)\n",
    "y_rgcn = np.array(label_rgcn).flatten()\n",
    "\n",
    "##############################################################\n",
    "\n",
    "mapping_dict_te = {}\n",
    "for file, x in zip(bankrupt_json_file, X_te[:50]):\n",
    "    mapping_dict_te[file] = x\n",
    "\n",
    "for file, x in zip(healthy_json_file, X_te[51:]):\n",
    "    mapping_dict_te[file] = x\n",
    "\n",
    "\n",
    "mapping_dict_re = {}\n",
    "for file, x in zip(bankrupt_json_file, X_re[:50]):\n",
    "    mapping_dict_re[file] = x\n",
    "\n",
    "for file, x in zip(healthy_json_file, X_re[51:]):\n",
    "    mapping_dict_re[file] = x\n",
    "\n",
    "mapping_dict_th = {}\n",
    "for file, x in zip(bankrupt_json_file, X_th[:50]):\n",
    "    mapping_dict_th[file] = x\n",
    "\n",
    "for file, x in zip(healthy_json_file, X_th[51:]):\n",
    "    mapping_dict_th[file] = x\n",
    "\n",
    "mapping_dict_ce = {}\n",
    "for file, x in zip(bankrupt_json_file, X_ce[:50]):\n",
    "    mapping_dict_ce[file] = x\n",
    "\n",
    "for file, x in zip(healthy_json_file, X_ce[51:]):\n",
    "    mapping_dict_ce[file] = x\n",
    "\n",
    "mapping_dict_rgcn = {}\n",
    "for file, x in zip(bankrupt_json_file, X_rgcn[:50]):\n",
    "    mapping_dict_rgcn[file] = x\n",
    "\n",
    "for file, x in zip(healthy_json_file, X_rgcn[51:]):\n",
    "    mapping_dict_rgcn[file] = x\n",
    "##############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TransE : Type <class 'numpy.float64'> | Shape (30,)\n",
      "RotatE : Type <class 'numpy.complex64'> | Shape (30,)\n",
      "TransH : Type <class 'numpy.float64'> | Shape (30,)\n",
      "ConvE : Type <class 'numpy.float64'> | Shape (30,)\n",
      "RGCN : Type <class 'numpy.float64'> | Shape (30,)\n"
     ]
    }
   ],
   "source": [
    "for (a, b, c, d, e) in zip(mapping_dict_te, mapping_dict_re, mapping_dict_th, mapping_dict_ce, mapping_dict_rgcn):\n",
    "    print(f\"TransE : Type {type(mapping_dict_te[a][0])} | Shape {mapping_dict_te[a].shape}\")\n",
    "    print(f\"RotatE : Type {type(mapping_dict_re[b][0])} | Shape {mapping_dict_re[b].shape}\")\n",
    "    print(f\"TransH : Type {type(mapping_dict_th[c][0])} | Shape {mapping_dict_th[c].shape}\")\n",
    "    print(f\"ConvE : Type {type(mapping_dict_ce[d][0])} | Shape {mapping_dict_ce[d].shape}\")\n",
    "    print(f\"RGCN : Type {type(mapping_dict_rgcn[e][0])} | Shape {mapping_dict_rgcn[e].shape}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run once\n",
    "os.makedirs('output/embeddings', exist_ok=True)\n",
    "\n",
    "def embedding_30(mapping_dict, model_name):\n",
    "    for key, value in mapping_dict.items():\n",
    "        mapping_dict[key] = value.tolist()\n",
    "\n",
    "    with open(f'output/embeddings/{model_name}.json', 'w') as f:\n",
    "        json.dump(mapping_dict, f)\n",
    "\n",
    "    df = pd.DataFrame.from_dict(mapping_dict, orient='index', columns=[f\"k{i}\" for i in range(1, 31)])\n",
    "    df.index.name = \"path\"\n",
    "    df.to_csv(f'output/embeddings/{model_name}_30.csv')\n",
    "\n",
    "embedding_30(mapping_dict_te, \"TransE\")\n",
    "# embedding_30(mapping_dict_re, \"RotatE\") # RotatE cannot be saved due to complex numbers\n",
    "embedding_30(mapping_dict_th, \"TransH\")\n",
    "embedding_30(mapping_dict_ce, \"ConvE\")\n",
    "embedding_30(mapping_dict_rgcn, \"RGCN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## <strong>Classification Test</strong>\n",
    "\n",
    "Direct classfication test by just using knowledge graph embedding models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TransE accuracy: 0.3\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.43      0.50      0.46         6\n",
      "           1       0.00      0.00      0.00         4\n",
      "\n",
      "    accuracy                           0.30        10\n",
      "   macro avg       0.21      0.25      0.23        10\n",
      "weighted avg       0.26      0.30      0.28        10\n",
      "\n",
      "TransH accuracy: 0.8\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.75      0.86         8\n",
      "           1       0.50      1.00      0.67         2\n",
      "\n",
      "    accuracy                           0.80        10\n",
      "   macro avg       0.75      0.88      0.76        10\n",
      "weighted avg       0.90      0.80      0.82        10\n",
      "\n",
      "ConvE accuracy: 0.9\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.80      0.89         5\n",
      "           1       0.83      1.00      0.91         5\n",
      "\n",
      "    accuracy                           0.90        10\n",
      "   macro avg       0.92      0.90      0.90        10\n",
      "weighted avg       0.92      0.90      0.90        10\n",
      "\n",
      "RGCN accuracy: 0.8\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      1.00      0.86         6\n",
      "           1       1.00      0.50      0.67         4\n",
      "\n",
      "    accuracy                           0.80        10\n",
      "   macro avg       0.88      0.75      0.76        10\n",
      "weighted avg       0.85      0.80      0.78        10\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##############################################\n",
    "# classification using logistic regression for different models\n",
    "##############################################\n",
    "X_train_te, X_test_te, y_train_te, y_test_te = train_test_split(X_te, y_te, test_size=0.2, shuffle=True)\n",
    "X_test_te, X_val_te, y_test_te, y_val_te = train_test_split(X_test_te, y_test_te, test_size=0.5, shuffle=True)\n",
    "clf_te = LogisticRegression(random_state=0).fit(X_train_te, y_train_te)\n",
    "\n",
    "y_pred_te = clf_te.predict(X_test_te)\n",
    "print(f\"TransE accuracy: {accuracy_score(y_test_te, y_pred_te)}\")\n",
    "print(classification_report(y_test_te, y_pred_te))\n",
    "\n",
    "##############################################\n",
    "X_train_th, X_test_th, y_train_th, y_test_th = train_test_split(X_th, y_th, test_size=0.2, shuffle=True)\n",
    "X_test_th, X_val_th, y_test_th, y_val_th = train_test_split(X_test_th, y_test_th, test_size=0.5, shuffle=True)\n",
    "clf_th = LogisticRegression(random_state=0).fit(X_train_th, y_train_th)\n",
    "\n",
    "y_pred_th = clf_th.predict(X_test_th)\n",
    "print(f\"TransH accuracy: {accuracy_score(y_test_th, y_pred_th)}\")\n",
    "print(classification_report(y_test_th, y_pred_th))\n",
    "\n",
    "##############################################\n",
    "X_train_ce, X_test_ce, y_train_ce, y_test_ce = train_test_split(X_ce, y_ce, test_size=0.2, shuffle=True)\n",
    "X_test_ce, X_val_ce, y_test_ce, y_val_ce = train_test_split(X_test_ce, y_test_ce, test_size=0.5, shuffle=True)\n",
    "clf_ce = LogisticRegression(random_state=0).fit(X_train_ce, y_train_ce)\n",
    "\n",
    "y_pred_ce = clf_ce.predict(X_test_ce)\n",
    "print(f\"ConvE accuracy: {accuracy_score(y_test_ce, y_pred_ce)}\")\n",
    "print(classification_report(y_test_ce, y_pred_ce))\n",
    "\n",
    "##############################################\n",
    "X_train_rgcn, X_test_rgcn, y_train_rgcn, y_test_rgcn = train_test_split(X_rgcn, y_rgcn, test_size=0.2, shuffle=True)\n",
    "X_test_rgcn, X_val_rgcn, y_test_rgcn, y_val_rgcn = train_test_split(X_test_rgcn, y_test_rgcn, test_size=0.5, shuffle=True)\n",
    "clf_rgcn = LogisticRegression(random_state=0).fit(X_train_rgcn, y_train_rgcn)\n",
    "\n",
    "y_pred_rgcn = clf_rgcn.predict(X_test_rgcn)\n",
    "print(f\"RGCN accuracy: {accuracy_score(y_test_rgcn, y_pred_rgcn)}\")\n",
    "print(classification_report(y_test_rgcn, y_pred_rgcn))\n",
    "##############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TransE model saved!\n",
      "TransH model saved!\n",
      "ConvE model saved!\n",
      "RGCN model saved!\n"
     ]
    }
   ],
   "source": [
    "os.makedirs('output/models', exist_ok=True)\n",
    "import pickle\n",
    "with open('output/models/TransE.pkl', 'wb') as f:\n",
    "    pickle.dump(clf_te, f)\n",
    "print(\"TransE model saved!\")\n",
    "\n",
    "with open('output/models/TransH.pkl', 'wb') as f:\n",
    "    pickle.dump(clf_th, f)\n",
    "print(\"TransH model saved!\")\n",
    "\n",
    "with open('output/models/ConvE.pkl', 'wb') as f:\n",
    "    pickle.dump(clf_ce, f)\n",
    "print(\"ConvE model saved!\")\n",
    "\n",
    "with open('output/models/RGCN.pkl', 'wb') as f:\n",
    "    pickle.dump(clf_rgcn, f)\n",
    "print(\"RGCN model saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ConvE** gave the best result for this run of 90% accuracy, \n",
    "**TransE** can also give better result if we try to run it multiple times\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "# **Neural Network**\n",
    "We use Neural network for further refining the embeddings into 2 vectors in 10 dimensional vector space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingModel(nn.Module):\n",
    "    def __init__(self, input_dim=30, embedding_dim=10):\n",
    "        super(EmbeddingModel, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(100),\n",
    "\n",
    "            nn.Linear(100, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(64),\n",
    "\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(32),\n",
    "\n",
    "            nn.Linear(32, embedding_dim),\n",
    "            nn.ReLU(),\n",
    "\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Linear(embedding_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeddings = self.model(x)\n",
    "        output = torch.sigmoid(self.classifier(embeddings))\n",
    "        return embeddings, output\n",
    "    \n",
    "model = EmbeddingModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################\n",
    "# 17/12/24 | 12:00 AM\n",
    "# NN model for refining embeddings \n",
    "# now the model is loading properly\n",
    "######################################################\n",
    "def embeddings_10(model_name, X_te, y_te):\n",
    "    X_train = torch.tensor(X_te, dtype=torch.float32)\n",
    "    y_train = torch.tensor(y_te, dtype=torch.float32)\n",
    "\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    num_epochs = 50\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        embeddings, outputs = model(X_train)\n",
    "        loss = criterion(outputs, y_train.view(-1, 1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if(epoch + 1) % 10 == 0:\n",
    "            print(f'Epoch: {epoch+1}, Loss: {loss.item():.4f}')\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        X_train_embeddings, _ = model(X_train)\n",
    "\n",
    "    torch.save(model.state_dict(), f\"output/models/nn_{model_name}.pth\")\n",
    "    print(\"Model saved successfully\")\n",
    "\n",
    "    ######################################################\n",
    "    mapping_dict_10 = {}\n",
    "\n",
    "    for file, x in zip(bankrupt_json_file, X_train[:50]):\n",
    "        mapping_dict_10[file], _ = model(x.unsqueeze(0))\n",
    "    for file, x in zip(healthy_json_file, X_train[51:]):\n",
    "        mapping_dict_10[file], _ = model(x.unsqueeze(0))\n",
    "\n",
    "    for key, value in mapping_dict_10.items():\n",
    "        mapping_dict_10[key] = value.tolist()[0]\n",
    "        \n",
    "    # convert into a csv file, easier to read\n",
    "    df = pd.DataFrame.from_dict(mapping_dict_10, orient='index',columns=[\"k1\", \"k2\", \"k3\", \"k4\", \"k5\", \"k6\", \"k7\", \"k8\", \"k9\", \"k10\"])\n",
    "    df.index.name = \"path\"\n",
    "    df.to_csv(f'output/embeddings/{model_name}_10.csv')\n",
    "######################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10, Loss: 0.2180\n",
      "Epoch: 20, Loss: 0.1666\n",
      "Epoch: 30, Loss: 0.1246\n",
      "Epoch: 40, Loss: 0.0921\n",
      "Epoch: 50, Loss: 0.0676\n",
      "Model saved successfully\n",
      "Epoch: 10, Loss: 0.4031\n",
      "Epoch: 20, Loss: 0.2255\n",
      "Epoch: 30, Loss: 0.1502\n",
      "Epoch: 40, Loss: 0.1045\n",
      "Epoch: 50, Loss: 0.0735\n",
      "Model saved successfully\n",
      "Epoch: 10, Loss: 0.1911\n",
      "Epoch: 20, Loss: 0.0995\n",
      "Epoch: 30, Loss: 0.0588\n",
      "Epoch: 40, Loss: 0.0371\n",
      "Epoch: 50, Loss: 0.0249\n",
      "Model saved successfully\n",
      "Epoch: 10, Loss: 0.2568\n",
      "Epoch: 20, Loss: 0.1049\n",
      "Epoch: 30, Loss: 0.0614\n",
      "Epoch: 40, Loss: 0.0400\n",
      "Epoch: 50, Loss: 0.0274\n",
      "Model saved successfully\n"
     ]
    }
   ],
   "source": [
    "embeddings_10(\"TransE\", X_te, y_te)\n",
    "embeddings_10(\"TransH\", X_th, y_th)\n",
    "embeddings_10(\"ConvE\", X_ce, y_ce)\n",
    "embeddings_10(\"RGCN\", X_rgcn, y_rgcn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
